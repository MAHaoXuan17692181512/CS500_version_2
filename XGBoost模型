import pandas as pd
import numpy as np
import warnings
from pathlib import Path
from typing import Dict, List, Optional, Callable
import os
import joblib
import matplotlib

matplotlib.use('TkAgg')  # ä¿ç•™æ‚¨çš„TkAggè®¾ç½®
import matplotlib.pyplot as plt
import seaborn as sns

matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Arial']
matplotlib.rcParams['axes.unicode_minus'] = False
warnings.filterwarnings('ignore')

try:
    import xgboost as xgb
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import mean_squared_error
except ImportError:
    warnings.warn("XGBoost/scikit-learn åº“æœªå®‰è£…ã€‚æ¨¡å‹è®­ç»ƒåŠŸèƒ½å°†æ— æ³•ä½¿ç”¨ã€‚")

warnings.filterwarnings('ignore')

FACTOR_FILE_EXTENSIONS = ['*.csv', '*.xlsx', '*.xls']


class XGBOOSTAnalyzer:

    def __init__(self, data_folder: str):
        self.data_folder = Path(data_folder)
        self.all_data = {}
        # self.synthetic_factors å­˜å‚¨åŠ è½½è¿›æ¥çš„å› å­å®½è¡¨æ•°æ®: {factor_name: pd.DataFrame}
        self.synthetic_factors = {}
        self.model = None
        # è®­ç»ƒé›†éœ€è¦ä»·æ ¼æ•°æ®æ¥è®¡ç®—æœªæ¥æ”¶ç›ŠYï¼Œæ‰€ä»¥ä¿ç•™è¿™äº›å­—æ®µ
        self.all_data['daily_close_return'] = None
        self.all_data['daily_open_return'] = None
        self.all_data['future_return_20d'] = None

    def validate_date_alignment(self, X_combined: pd.DataFrame, y_series: pd.Series) -> bool:
        """
        éªŒè¯ç‰¹å¾Xå’Œç›®æ ‡Yçš„æ—¥æœŸå¯¹é½æƒ…å†µ
        """
        print(f"\nğŸ“… æ­£åœ¨éªŒè¯æ—¥æœŸå¯¹é½æƒ…å†µ...")

        # è·å–Xå’ŒYçš„æ—¥æœŸé›†åˆ
        X_dates = set(X_combined.index.get_level_values('date'))
        y_dates = set(y_series.index.get_level_values('date'))

        print(f"  ç‰¹å¾Xæ—¥æœŸèŒƒå›´: {min(X_dates)} åˆ° {max(X_dates)}")
        print(f"  ç›®æ ‡Yæ—¥æœŸèŒƒå›´: {min(y_dates)} åˆ° {max(y_dates)}")
        print(f"  ç‰¹å¾Xå”¯ä¸€æ—¥æœŸæ•°: {len(X_dates)}")
        print(f"  ç›®æ ‡Yå”¯ä¸€æ—¥æœŸæ•°: {len(y_dates)}")

        # è®¡ç®—é‡å æƒ…å†µ
        overlap_dates = X_dates & y_dates
        overlap_ratio = len(overlap_dates) / min(len(X_dates), len(y_dates)) if min(len(X_dates),
                                                                                    len(y_dates)) > 0 else 0

        print(f"  é‡å æ—¥æœŸæ•°: {len(overlap_dates)}")
        print(f"  æ—¥æœŸé‡å åº¦: {overlap_ratio:.2%}")

        if overlap_ratio == 0:
            print("âŒ ä¸¥é‡é”™è¯¯ï¼šç‰¹å¾å’Œç›®æ ‡å˜é‡æ—¥æœŸå®Œå…¨æ— é‡å ï¼")
            return False
        elif overlap_ratio < 0.5:
            print("âš ï¸ è­¦å‘Šï¼šç‰¹å¾å’Œç›®æ ‡å˜é‡æ—¥æœŸé‡å åº¦è¾ƒä½ (<50%)")
            return True
        else:
            print("âœ… æ—¥æœŸå¯¹é½éªŒè¯é€šè¿‡")
            return True

    # --- parse_dirty_wide_table å’Œ load_all_periods ä¿æŒä¸å˜ ---
    def parse_dirty_wide_table(self, raw_data: pd.DataFrame, sheet_name: str = None) -> pd.DataFrame:
        try:
            data = raw_data.copy()
            data.columns = data.iloc[0].values
            clean_data = data.iloc[2:].copy()
            time_col_name = clean_data.columns[0]
            clean_data = clean_data.set_index(time_col_name)
            cleaned_index = [val if not isinstance(val, tuple) else pd.NaT for val in clean_data.index]
            clean_data.index = pd.Index(cleaned_index)
            clean_data.index = pd.to_datetime(clean_data.index)
            clean_data = clean_data.apply(pd.to_numeric, errors='coerce')
            clean_data = clean_data.dropna(axis=1, how='all')
            clean_data = clean_data.replace(0, np.nan)
            clean_data = clean_data.fillna(method='ffill')
            clean_data.columns.name = 'code'
            clean_data.index.name = 'date'
            return clean_data
        except Exception as e:
            print(f"âŒ è§£æ {sheet_name} å¤±è´¥: {e}")
            raise

    def load_all_periods(self):
        excel_files = sorted(
            list(self.data_folder.glob("ä¸­è¯500_*.xlsx")) +
            list(self.data_folder.glob("ä¸­è¯500_*.xls"))
        )
        if not excel_files:
            # å¯¹äºæµ‹è¯•é›†ï¼Œå¯èƒ½ä¸éœ€è¦ä»·æ ¼æ•°æ®ï¼Œæ‰€ä»¥è¿™ä¸ªè­¦å‘Šå¯ä»¥æ¥å—
            print(f"âš ï¸ æœªæ‰¾åˆ°æœ¬åœ°ä»·æ ¼ Excel æ–‡ä»¶ã€‚")
            return
        print(f"æ‰¾åˆ° {len(excel_files)} ä¸ªæ•°æ®æ–‡ä»¶")
        all_close_prices = []
        all_open_prices = []
        for file in excel_files:
            period_name = file.stem.replace("ä¸­è¯500_", "")
            try:
                xl_file = pd.ExcelFile(file)
                period_data = {}
                for sheet_name in xl_file.sheet_names:
                    if sheet_name != 'Sheet1':
                        sheet_data = pd.read_excel(file, sheet_name=sheet_name, header=None)
                        try:
                            parsed_data = self.parse_dirty_wide_table(sheet_data, sheet_name)
                            if not parsed_data.empty: period_data[sheet_name] = parsed_data
                        except Exception:
                            continue
                self.all_data[period_name] = period_data
                if 'æ”¶ç›˜ä»·' in period_data: all_close_prices.append(period_data['æ”¶ç›˜ä»·'])
                if 'å¼€ç›˜ä»·' in period_data: all_open_prices.append(period_data['å¼€ç›˜ä»·'])
            except Exception as e:
                print(f"âŒ åŠ è½½æ–‡ä»¶ {file.name} å¤±è´¥: {e}")

        if all_close_prices:
            all_close_prices_df = pd.concat(all_close_prices).sort_index()
            self.all_data['daily_close_return'] = all_close_prices_df.pct_change().stack().rename('daily_return')
            print("ğŸ“ˆ å·²åˆå¹¶æ‰€æœ‰æ—¥æ”¶ç›˜æ”¶ç›Šç‡ã€‚")

        if all_open_prices:
            all_open_prices_df = pd.concat(all_open_prices).sort_index()
            daily_open_return_df = all_open_prices_df.pct_change()
            self.all_data['daily_open_return'] = daily_open_return_df.stack().rename('daily_return')

            past_cum_return_20d = daily_open_return_df.apply(
                lambda col: col.rolling(window=20).apply(lambda x: np.prod(1 + x) - 1, raw=True)
            )
            future_return_20d = past_cum_return_20d.shift(-20)
            self.all_data['future_return_20d'] = future_return_20d.stack().rename('future_return')
            print("âœ… 20 æ—¥æ»šåŠ¨æœªæ¥æ”¶ç›Šç›®æ ‡ (Y) å·²è®¡ç®—å¹¶å­˜å‚¨ã€‚")

        # ä¿è¯ _display_data_overview å­˜åœ¨
        if hasattr(self, '_display_data_overview') and self.all_data:
            self._display_data_overview()

    def _display_data_overview(self):
        print(f"\n{'=' * 60}")
        print("æ•°æ®åŠ è½½æ¦‚è§ˆ")
        print(f"{'=' * 60}")
        # ä»…æ˜¾ç¤ºåŠ è½½çš„æ•°æ®é¡¹
        for period_name, period_data in self.all_data.items():
            if isinstance(period_data, dict):
                print(f"\nğŸ“… æ—¶é—´æ®µ: {period_name} - åŒ…å« {len(period_data)} ä¸ªæ•°æ®è¡¨")

    # --- load_precalculated_factors ä¿æŒä¸å˜ ---
    def load_precalculated_factors(self, factor_folder: str):
        """ä»æŒ‡å®šæ–‡ä»¶å¤¹åŠ è½½é¢„è®¡ç®—çš„å› å­æ•°æ® (Date Index, Code Columns çš„å®½è¡¨)"""
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ self.data_folder æ¥æŸ¥æ‰¾æ–‡ä»¶ï¼Œå¦‚æœ factor_folder æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„è·¯å¾„ï¼Œ
        # åº”è¯¥ä¿®æ”¹æˆä½¿ç”¨ factor_folder.glob(ext)
        folder = Path(factor_folder)  # ä½¿ç”¨ä¼ å…¥çš„å› å­è·¯å¾„
        factor_files = []
        for ext in FACTOR_FILE_EXTENSIONS:
            factor_files.extend(list(folder.glob(ext)))
        # ... (ä¸åŸå§‹å®ç°ç›¸åŒï¼Œçœç•¥) ...
        # (ä¿ç•™ load_precalculated_factors çš„åŸå§‹å®ç°)

        # ç®€åŒ–åçš„ load_precalculated_factorsï¼Œå‡è®¾å®ƒä½¿ç”¨ self.data_folder æ¥æŸ¥æ‰¾å› å­æ–‡ä»¶ï¼š

        # ä¸ºäº†å…¼å®¹æ€§ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„ factor_folder ä½œä¸ºæŸ¥æ‰¾è·¯å¾„
        folder = Path(factor_folder)
        factor_files = []
        for ext in FACTOR_FILE_EXTENSIONS:
            # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶
            factor_files.extend(list(folder.glob(ext)))

        factor_files = [f for f in factor_files if f.stem not in ['summary', 'readme']]  # ç®€å•è¿‡æ»¤

        if not factor_files:
            print(f"âš ï¸ åœ¨è·¯å¾„ {factor_folder} ä¸­æœªæ‰¾åˆ°ä»»ä½•å› å­æ–‡ä»¶ã€‚")
            return

        print(f"\næ‰¾åˆ° {len(factor_files)} ä¸ªé¢„è®¡ç®—å› å­æ–‡ä»¶ï¼Œå¼€å§‹åŠ è½½...")
        self.synthetic_factors = {}

        for file in factor_files:
            factor_name = file.stem.replace('_panel_data', '').replace('_WIDE_DATA', '')  # æ¸…ç†å› å­å

            try:
                if file.suffix in ['.xlsx', '.xls']:
                    factor_df = pd.read_excel(file)
                elif file.suffix == '.csv':
                    factor_df = pd.read_csv(file)
                else:
                    continue

                if factor_df.empty: continue

                # å°è¯•å°†é•¿è¡¨ (Panel Data) è½¬æ¢ä¸ºå®½è¡¨ (Wide Data)
                if 'date' in factor_df.columns and 'code' in factor_df.columns:
                    if factor_name not in factor_df.columns:
                        value_col = [col for col in factor_df.columns if col not in ['date', 'code']][0]
                        factor_df = factor_df.rename(columns={value_col: factor_name})
                    factor_df = factor_df.drop_duplicates(subset=['date', 'code'], keep='first')
                    factor_df = factor_df.pivot(index='date', columns='code', values=factor_name)
                    factor_df.index = pd.to_datetime(factor_df.index)
                    factor_df.columns = factor_df.columns.astype(str)
                else:
                    # å‡è®¾å·²ç»æ˜¯å®½è¡¨æ ¼å¼ï¼Œç¬¬ä¸€åˆ—æ˜¯æ—¥æœŸç´¢å¼•
                    factor_df = pd.read_excel(file, index_col=0) if file.suffix in ['.xlsx', '.xls'] else pd.read_csv(
                        file, index_col=0)
                    factor_df.index = pd.to_datetime(factor_df.index)
                    factor_df.columns = factor_df.columns.astype(str)

                # æœ€åæ¸…ç†
                factor_df = factor_df.apply(pd.to_numeric, errors='coerce')
                factor_df = factor_df.dropna(axis=1, how='all')
                factor_df.columns.name = 'code'
                factor_df.index.name = 'date'

                self.synthetic_factors[factor_name] = factor_df
                print(f"    âœ… åŠ è½½å¹¶è½¬æ¢ä¸ºå®½è¡¨å› å­: {factor_name} (Shape: {factor_df.shape})")

            except Exception as e:
                print(f"âŒ åŠ è½½æ–‡ä»¶ {file.name} å¤±è´¥: {e}")
                continue

    def prepare_xgboost_data(self, lookback_days: int = 20, predict_days: int = 20) -> Optional[pd.DataFrame]:
        """
        æ ¸å¿ƒæ•°æ®å‡†å¤‡é€»è¾‘ï¼Œç”¨äºè®¡ç®— X (æ»šåŠ¨å‡å€¼) å¹¶ä¸ Y (æœªæ¥æ”¶ç›Š) åˆå¹¶ã€‚
        å¯¹äºæµ‹è¯•é›†ï¼Œå¦‚æœ Y ä¸å­˜åœ¨ï¼Œå®ƒä¼šå°è¯•ä½¿ç”¨ä¸€ä¸ªç©ºå ä½ç¬¦æ¥ç”Ÿæˆ Xã€‚
        """
        print(f"\nâš™ï¸ æ­£åœ¨å‡†å¤‡ XGBoost æ•°æ®é›† (X: è¿‡å» {lookback_days} å¤©å¹³å‡, Y: æœªæ¥ {predict_days} å¤©æ”¶ç›Š)...")

        y_series = self.all_data.get('future_return_20d')
        is_training_set = y_series is not None and not y_series.empty

        if not self.synthetic_factors:
            print("âŒ é”™è¯¯ï¼šå› å­æ•°æ®ç¼ºå¤±ã€‚è¯·æ£€æŸ¥åŠ è½½æ˜¯å¦æˆåŠŸã€‚")
            return None

        factor_dfs = []

        # 1. å¯¹æ¯ä¸ªå› å­è¿›è¡Œ lookback_days (20å¤©) æ»šåŠ¨å‡å€¼è®¡ç®—
        for factor_name, wide_factor_df in self.synthetic_factors.items():
            X_df_rolled = wide_factor_df.apply(
                lambda col: col.rolling(window=lookback_days, min_periods=1).mean()
            )
            X_df_feature = X_df_rolled.shift(1)
            X_series_feature = X_df_feature.stack().rename(factor_name)
            factor_dfs.append(X_series_feature)

        # 2. åˆå¹¶æ‰€æœ‰æ»šåŠ¨åçš„å› å­ç‰¹å¾ (X)
        X_combined = pd.concat(factor_dfs, axis=1)
        X_combined.index.names = ['date', 'code']  # ç¡®ä¿ MultiIndex åç§°æ­£ç¡®
        X_combined = X_combined.fillna(0)

        if is_training_set:
            # 3a. è®­ç»ƒé›†ï¼šåˆå¹¶ç‰¹å¾ X å’Œç›®æ ‡ Y (å†…è¿æ¥å¯¹é½)
            if not self.validate_date_alignment(X_combined, y_series):
                print("âŒ æ—¥æœŸå¯¹é½éªŒè¯å¤±è´¥ï¼Œæ— æ³•ç»§ç»­è®­ç»ƒ")
                return None

            combined_df = X_combined.join(y_series, how='inner')
            combined_df.dropna(subset=['future_return'], inplace=True)
            # combined_df.dropna(inplace=True)
            print(f"âœ… è®­ç»ƒé›†æ•°æ®å‡†å¤‡å®Œæˆã€‚æ€»æ ·æœ¬æ•°: {len(combined_df)}")
        else:
            # 3b. æµ‹è¯•é›†ï¼šåªä¿ç•™ç‰¹å¾ X
            combined_df = X_combined.dropna()
            print(f"âœ… æµ‹è¯•é›†ç‰¹å¾å‡†å¤‡å®Œæˆã€‚æ€»æ ·æœ¬æ•°: {len(combined_df)}")

        return combined_df.reset_index(names=['date', 'code'])

    # train_xgboost_model ä¿æŒåŸæ ·ï¼Œä½† main å‡½æ•°ä¸­å°†è·³è¿‡å®ƒ
    def train_xgboost_model(self, data_df: pd.DataFrame, model_path: str = 'XGBOOST_20Day_Factor_Model.json'):
        print("\nğŸ§  æ­£åœ¨è®­ç»ƒ XGBoost æ¨¡å‹ ...")
        X = data_df.drop(columns=['date', 'code', 'future_return'])
        y = data_df['future_return']
        if X.empty:
            print("âŒ è®­ç»ƒæ•°æ®é›†ä¸ºç©ºã€‚")
            return
        try:
            model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, max_depth=5,
                                     random_state=42, n_jobs=-1)
            # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)
            model.fit(X, y)
            y_pred = model.predict(X)
            mse = mean_squared_error(y, y_pred)
            rmse = np.sqrt(mse)
            print(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆã€‚æµ‹è¯•é›† RMSE: {rmse:.4f}")
            self.model = model
        except NameError:
            print("âš ï¸ XGBoost åº“æœªå®‰è£…ï¼Œè·³è¿‡æ¨¡å‹è®­ç»ƒã€‚")
        except Exception as e:
            print(f"âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")


# ====================================================================
# ã€æ–°å¢ã€‘ç”¨äºå¤„ç†æµ‹è¯•é›†å› å­çš„é¢„æµ‹å™¨ç±»
# ====================================================================

class XGBoostPredictor(XGBOOSTAnalyzer):
    """
    ä¸“ä¸ºæµ‹è¯•é›†é¢„æµ‹è®¾è®¡ï¼Œç»§æ‰¿è‡ª Analyzerï¼Œç”¨äºåŠ è½½æµ‹è¯•é›†å› å­çš„ç‰¹å¾æ•°æ® Xã€‚
    """

    def __init__(self, test_folder: str):
        super().__init__(data_folder=test_folder)  # data_folder æŒ‡å‘ test_folder

    def load_test_factors_only(self):
        """åªåŠ è½½æµ‹è¯•é›†çš„å› å­æ–‡ä»¶ã€‚"""
        # test_folder ç°åœ¨å°±æ˜¯å› å­æ‰€åœ¨çš„è·¯å¾„
        super().load_precalculated_factors(factor_folder=str(self.data_folder))

        # è¦†ç›–çˆ¶ç±»çš„ prepare_xgboost_dataï¼Œä½†ç”±äºçˆ¶ç±»å·²ç»ä¿®æ”¹ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥è°ƒç”¨å®ƒ

    def prepare_test_data(self, lookback_days: int = 20, predict_days: int = 20) -> Optional[pd.DataFrame]:
        """
        å‡†å¤‡æµ‹è¯•é›†ç‰¹å¾æ•°æ® Xã€‚
        """
        # çˆ¶ç±» prepare_xgboost_data ç°åœ¨å¯ä»¥å¤„ç† self.all_data['future_return_20d'] ä¸ºç©ºçš„æƒ…å†µ
        return super().prepare_xgboost_data(lookback_days=lookback_days, predict_days=predict_days)


# plot_feature_importance å‡½æ•°ä¿æŒä¸å˜
def plot_feature_importance(
        model: xgb.XGBRegressor,
        feature_names: List[str],
        importance_type: str = 'gain',
        top_n: int = 15
):
    # ... (ä¸ä¹‹å‰æä¾›çš„å®ç°ç›¸åŒ) ...
    print(f"\n--- ğŸ“ˆ å˜é‡è´¡çŒ®åº¦åˆ†æ (åŸºäº {importance_type.upper()}) ---")

    try:
        importance_scores = model.get_booster().get_score(
            importance_type=importance_type
        )
    except xgb.core.XGBoostError as e:
        print(f"âŒ è·å–é‡è¦æ€§å¾—åˆ†å¤±è´¥ï¼Œè¯·ç¡®è®¤æ¨¡å‹å·²è®­ç»ƒï¼š{e}")
        return

    importance_df = pd.DataFrame(
        list(importance_scores.items()),
        columns=['Feature', f'Importance_{importance_type.capitalize()}']
    )

    if all(name.startswith('f') and name[1:].isdigit() for name in importance_df['Feature']):
        name_map = {f'f{i}': name for i, name in enumerate(feature_names)}
        importance_df['Feature'] = importance_df['Feature'].map(name_map).fillna(importance_df['Feature'])

    importance_df = importance_df.sort_values(
        by=f'Importance_{importance_type.capitalize()}',
        ascending=False
    ).head(top_n)

    print(importance_df)

    plt.figure(figsize=(10, 6))
    sns.barplot(
        x=f'Importance_{importance_type.capitalize()}',
        y='Feature',
        data=importance_df,
        palette='viridis'
    )
    plt.title(f'XGBoost å˜é‡è´¡çŒ®åº¦ ({importance_type.capitalize()}) - Top {top_n}', fontsize=14)
    plt.xlabel(f'è´¡çŒ®åº¦ ({importance_type.capitalize()})', fontsize=12)
    plt.ylabel('å˜é‡', fontsize=12)
    plt.tight_layout()
    plt.show()


# ====================================================================
#                          ç¨‹åºæ‰§è¡Œå— (Main)
# ====================================================================

def main():
    # è·¯å¾„é…ç½®
    raw_data_folder = "C:/Users/cufet/Desktop/è®­ç»ƒé›†"
    test_folder = 'C:/Users/cufet/Desktop/æµ‹è¯•é›†'  # åŒ…å«æµ‹è¯•æ—¥å› å­æ–‡ä»¶çš„è·¯å¾„
    factor_folder_path = "C:/Users/cufet/Desktop/Factor_Analysis_Output"
    OUTPUT_EXCEL_PATH = "C:/Users/cufet/Desktop/XGBoost_Test_Predictions.xlsx"  # è¾“å‡ºæ–‡ä»¶åæ”¹ä¸º Test_Predictions

    # -----------------------------------------------------------
    #                       è®­ç»ƒé˜¶æ®µ
    # -----------------------------------------------------------
    analyzer = XGBOOSTAnalyzer(data_folder=raw_data_folder)

    # 1. åŠ è½½è®­ç»ƒé›†ä»·æ ¼æ•°æ® (ç”¨äºè®¡ç®— Y)
    analyzer.load_all_periods()

    # 2. åŠ è½½è®­ç»ƒé›†å› å­å®½è¡¨æ•°æ® (ç”¨äºè®¡ç®— X)
    analyzer.load_precalculated_factors(factor_folder_path)

    # 3. å‡†å¤‡ XGBoost è®­ç»ƒé›† (X + Y)
    lookback_days = 20
    predict_days = 20
    xgboost_data = analyzer.prepare_xgboost_data(lookback_days=lookback_days, predict_days=predict_days)

    if xgboost_data is None or xgboost_data.empty:
        print("âŒ è®­ç»ƒæ•°æ®é›†å‡†å¤‡å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚")
        return

    # 4. è®­ç»ƒæ¨¡å‹ (ä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®æ‹Ÿåˆï¼Œè·³è¿‡ train_test_split åˆ’åˆ†)
    X_train_all = xgboost_data.drop(columns=['date', 'code', 'future_return'])
    y_train_all = xgboost_data['future_return']
    X_features = X_train_all.columns.tolist()

    print("\nğŸ§  æ­£åœ¨ä½¿ç”¨å…¨éƒ¨è®­ç»ƒé›†æ•°æ®è®­ç»ƒ XGBoost æ¨¡å‹...")

    try:
        model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, max_depth=5,
                                 random_state=42, n_jobs=-1)
        model.fit(X_train_all, y_train_all)
        analyzer.model = model
        y_pred_train = analyzer.model.predict(X_train_all)
        rmse_train = np.sqrt(mean_squared_error(y_train_all, y_pred_train))
        print(f"âœ… æ¨¡å‹è®­ç»ƒ/æ‹Ÿåˆå®Œæˆã€‚**è®­ç»ƒé›†** RMSE: {rmse_train:.4f}")
    except Exception as e:
        print(f"âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
        return

    # 5. è¿›è¡Œå˜é‡è´¡çŒ®åº¦åˆ†æå’Œå¯è§†åŒ– (ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹)
    plot_feature_importance(
        model=analyzer.model,
        feature_names=X_features,
        importance_type='gain',
        top_n=15
    )

    # -----------------------------------------------------------
    #                       é¢„æµ‹é˜¶æ®µï¼šåœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹
    # -----------------------------------------------------------
    if analyzer.model is not None:

        # 1. å®ä¾‹åŒ–é¢„æµ‹å™¨ï¼ŒæŒ‡å‘æµ‹è¯•é›†å› å­æ–‡ä»¶å¤¹
        predictor = XGBoostPredictor(test_folder=test_folder)

        # 2. åŠ è½½æµ‹è¯•é›†å› å­æ•°æ®
        predictor.load_test_factors_only()

        # 3. å‡†å¤‡æµ‹è¯•é›†çš„ç‰¹å¾ X (æ³¨æ„ï¼šè¿™é‡Œ Y ç¼ºå¤±ï¼Œprepare_xgboost_data åªä¼šè¿”å› X)
        test_X_df_with_index = predictor.prepare_test_data(lookback_days=lookback_days, predict_days=predict_days)

        if test_X_df_with_index is None or test_X_df_with_index.empty:
            print("âŒ æµ‹è¯•é›†ç‰¹å¾å‡†å¤‡å¤±è´¥æˆ–æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡é¢„æµ‹ã€‚")
            return

        # åˆ†ç¦»ç´¢å¼•å’Œç‰¹å¾çŸ©é˜µ
        test_index_df = test_X_df_with_index[['date', 'code']].copy()
        test_X_matrix = test_X_df_with_index.drop(columns=['date', 'code'])

        # 4. ç¡®ä¿æµ‹è¯•é›†ç‰¹å¾é¡ºåºä¸è®­ç»ƒé›†ä¸€è‡´ (è¿™æ˜¯å…³é”®æ­¥éª¤ï¼)
        try:
            test_X_matrix = test_X_matrix[X_features]
        except KeyError as e:
            print(f"âŒ é”™è¯¯ï¼šæµ‹è¯•é›†å› å­ä¸è®­ç»ƒé›†å› å­ä¸åŒ¹é…ã€‚ç¼ºå¤±ç‰¹å¾: {e}")
            return

        # 5. è¿›è¡Œé¢„æµ‹
        print("\n--- ğŸš€ æ­£åœ¨å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ ---")
        y_pred_test = analyzer.model.predict(test_X_matrix)

        # 6. æ„å»ºé¢„æµ‹ç»“æœ DataFrame
        test_results_df = test_index_df.copy()  # åŒ…å« date å’Œ code

        # ç¡®ä¿é¢„æµ‹å€¼å’Œç´¢å¼•é•¿åº¦åŒ¹é… (ä¿æŒä¸å˜)
        if len(y_pred_test) != len(test_results_df):
            # ... (é”™è¯¯å¤„ç†) ...
            return
        test_results_df['Predicted_Return'] = y_pred_test
        # ğŸŒŸğŸŒŸ æ ¸å¿ƒä¿®æ”¹ï¼šé€è§†å¹¶é‡ç½®ç´¢å¼• ğŸŒŸğŸŒŸ
        try:
            wide_results_df = test_results_df.pivot(
                index='date',
                columns='code',
                values='Predicted_Return'
            )

            # ğŸŒŸğŸŒŸ æœ€ç»ˆä¿®å¤ç‚¹ï¼šå¼ºåˆ¶æ¸…é™¤åˆ—ç´¢å¼•åç§° 'code' ğŸŒŸğŸŒŸ
            # 1. ç¡®ä¿åˆ—åï¼ˆè‚¡ç¥¨ä»£ç ï¼‰æ˜¯å­—ç¬¦ä¸²
            wide_results_df.columns = wide_results_df.columns.astype(str)

            # 2. ã€æœ€å…³é”®ã€‘å½»åº•ç§»é™¤åˆ—ç´¢å¼•åç§°ï¼Œè¿™æ˜¯å¯¼è‡´å†²çªçš„æ ¹æº
            wide_results_df.columns.name = None

            # 3. ç¡®ä¿è¡Œç´¢å¼•åç§° 'date' è¢«ä¿ç•™ (å¯é€‰ï¼Œä½†æ¨èä¿ç•™æ¸…æ™°æ€§)
            wide_results_df.index.name = 'date'

        except KeyError as e:
            print(f"âŒ å…³é”®åˆ— '{e}' ç¼ºå¤±ã€‚è¯·æ£€æŸ¥ y_pred_test æ˜¯å¦ä¸ºç©ºæˆ–é•¿åº¦ä¸åŒ¹é…ã€‚")
            return
        # 7. å¯¼å‡ºåˆ° Excel
        try:
            wide_results_df.to_excel(OUTPUT_EXCEL_PATH)
            print(f"âœ… **æµ‹è¯•é›†**é¢„æµ‹ç»“æœï¼ˆå®½è¡¨ï¼ŒåŒ…å«æ—¥æœŸï¼‰å·²æˆåŠŸå¯¼å‡ºåˆ°: {OUTPUT_EXCEL_PATH}")
            print(f"   å¯¼å‡ºæ ·æœ¬æ€»æ•°: {len(wide_results_df.stack().dropna())} / æ—¥æœŸæ•°: {len(wide_results_df)}")
        except Exception as e:
            print(f"âŒ å¯¼å‡ºæµ‹è¯•é›† Excel å¤±è´¥: {e}")

    print("\nğŸ‰ æ•°æ®å‡†å¤‡ä¸é¢„æµ‹æµç¨‹æ‰§è¡Œå®Œæ¯•ã€‚")


if __name__ == "__main__":
    main()
